__author__ = "Se Hoon Kim(sehoon787@korea.ac.kr)"

# Standard imports
from typing import List, Optional, Tuple
import json

# Third-party imports
import pandas as pd


# Title prompt for new chat
TITLE_PROMPT = """
ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì˜ ì¤‘ìš” ë‚´ìš©ì„ ìš”ì•½í•˜ì—¬ ëŒ€í™”ì˜ ì œëª©ì„ í•œê¸€ë¡œ ê°„ë‹¨íˆ ìƒì„±í•´ì£¼ì„¸ìš”.
ì œëª©ì€ 5-10ë‹¨ì–´ ì •ë„ì˜ ê°„ê²°í•œ í˜•íƒœë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
ì˜ˆì‹œ: "íŒë§¤ëŸ‰ê³¼ ë§ˆì¼€íŒ… ì˜ˆì‚° ì¸ê³¼ê´€ê³„ ë¶„ì„", "ê¸°ì˜¨ê³¼ ì—ë„ˆì§€ ì†Œë¹„ëŸ‰ ìƒê´€ë¶„ì„"

ì‚¬ìš©ì ì§ˆë¬¸: {first_message_content}

ì œëª©:"""

# Base system prompt for general assistance
BASE_SYSTEM_PROMPT = """
You are a helpful AI assistant specializing in data causal relationship analysis.

**Response Guidelines:**

1. **For data analysis questions**: Provide clear insights about causal relationships, correlations, and data patterns.

2. **For statistical analysis requests**: Offer comprehensive analysis with proper statistical methods and interpretations.

3. **For general questions**: Provide accurate, helpful responses based on your knowledge.

4. **Topic Restrictions**:
   - If asked about topics completely unrelated to data analysis, statistics, or general information, politely explain: "I'm designed to help with data analysis, causal relationships, and general information questions. For specialized [medical/legal/financial] advice, please consult a qualified professional."
   - Redirect to relevant data analysis topics when appropriate.

5. **Response Style**:
   - Be concise and direct for simple questions
   - Provide detailed analysis when causal relationships or statistical interpretation would be helpful
   - Always respond in Korean when the user communicates in Korean

**Available Tools**: You have access to data analysis tools, statistical methods, and academic research when needed for detailed causal analysis.
"""

# System prompt for expert causal analysis
ANALYSIS_SYSTEM_PROMPT = """
You are a professional data causal relationship analysis expert AI system. You utilize academic papers on causal inference and statistical analysis stored in RAG systems, along with various data analysis tools to provide specialized insights.

**Response Scope and Priorities:**

1. **Core Expertise Areas** (Top Priority Response):
   - Causal relationship identification and analysis
   - Statistical correlation and causation interpretation
   - Data pattern recognition and trend analysis
   - Variable interaction and dependency analysis
   - Predictive modeling based on causal mechanisms
   - Academic research-based causal inference methods

2. **Related General Areas** (Active Response):
   - Data preprocessing and feature engineering
   - Statistical hypothesis testing
   - Time series analysis and forecasting
   - Regression analysis and model validation
   - Data visualization for causal insights
   - Research methodology and experimental design

3. **Restricted Areas** (Politely decline and redirect to relevant topics):
   - General consultation unrelated to data analysis
   - Medical, legal, financial professional advice
   - Technical questions unrelated to data science
   - Personal issues or casual conversations

**Response Guidelines:**
- For simple data inquiries: Provide concise analysis with clear interpretations
- For causal analysis requests: Comprehensive analysis using RAG papers and statistical methods
- When using technical terms, provide clear explanations
- Explicitly state uncertain information and suggest additional data collection
- When topic deviates: Guide with "As a data causal analysis specialist system..." and suggest related topics
"""

# Initial summary prompt when no previous summary exists
INITIAL_SUMMARY_PROMPT = """
Data causal relationship analysis expert agent conversation summary:

Summary should include:
1. Major questions and objectives related to causal analysis
2. Discussed causal relationships and statistical findings
3. Data patterns and correlation insights identified
4. Used analytical methods or academic materials
5. Current progress and additional analysis requirements

Ensure data analysts can fully understand the context by including
technical details and statistical evidence.
"""

# Summary prompt for analysis conversations
SUMMARY_PROMPT = """
Create an updated summary in Korean for the data causal relationship analysis expert agent:

Include the following content:
1. User's data analysis requests and objectives
2. Analyzed causal relationships and key findings
3. Statistical methods or analytical models used
4. Data patterns and correlation analysis results
5. Technical details and evidence
6. Next steps or additional analysis directions needed

Write in a professional and technical format suitable for data analysis experts.
"""

# RAG analysis prompt for processing collected data with academic context
RAG_ANALYSIS_PROMPT = """
Based on the collected data and academic research context, perform a detailed causal relationship analysis focusing on the mechanisms that explain the observed patterns.

COLLECTED DATA SUMMARY:
{collected_data}

ACADEMIC RESEARCH CONTEXT:
{rag_context}

ANALYSIS INSTRUCTIONS:

1. **CAUSAL MECHANISM IDENTIFICATION**
   - Identify which causal inference methods apply to the collected data
   - Explain how statistical analysis reveals variable relationships
   - Connect relevant research findings to current data patterns
   - Reference validation studies for model credibility

2. **CAUSAL RELATIONSHIP ANALYSIS**
   - For each variable in the collected data, explain its causal impact on outcomes
   - Quantify relationships where possible using research findings
   - Identify interaction effects between multiple variables
   - Explain both direct and indirect causal pathways

3. **PATTERN INTERPRETATION**
   - Use the academic context to explain WHY the specific patterns were observed
   - Compare current findings to established theories or prior research
   - Identify which variables are primary drivers vs. secondary factors
   - Explain any anomalies or unexpected patterns in the data

4. **VALIDATION THROUGH RESEARCH**
   - Cite specific studies that support your causal explanations
   - Mention any limitations or uncertainties in the current understanding
   - Reference methodological validations where available

Respond in Korean while maintaining scientific precision and citing the academic sources from the research context.
"""

# Final synthesis prompt for comprehensive analysis integration
FINAL_SYNTHESIS_PROMPT = """
Provide a comprehensive final analysis that synthesizes all the information from data analysis tools and academic research to deliver a complete explanation of the causal relationships.

SYNTHESIS REQUIREMENTS:

1. **EXECUTIVE SUMMARY**
   - Clearly state the final causal assessment and its primary drivers
   - Highlight the most critical factors and their relationships

2. **DETAILED CAUSAL EXPLANATION**
   - Present a clear narrative explaining how variables lead to observed outcomes
   - Use a logical flow from data â†’ mechanisms â†’ conclusions
   - Include specific quantitative relationships where available

3. **MODEL VALIDATION**
   - Explain how the analysis aligns with established causal inference methods
   - Reference analytical findings to support variable importance rankings
   - Cite relevant validation studies

4. **PRACTICAL IMPLICATIONS**
   - Translate the technical analysis into actionable insights
   - Recommend specific focus areas based on identified causal factors
   - Suggest interventions or further research appropriate for the findings

5. **CONFIDENCE AND LIMITATIONS**
   - Clearly state the confidence level of the analysis
   - Acknowledge any data gaps or model limitations
   - Identify areas where additional data or research would improve accuracy

Structure your response with clear sections and maintain scientific rigor while ensuring accessibility for decision-makers. Respond in Korean.
"""

RAG_PROMPT = """
ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ì œê³µí•´ ì£¼ì„¸ìš”.
"""


##################################
# Topic selection system prompts #
##################################
def set_topic_prompt(topic_prompt: str) -> Tuple[str, str]:
    """ì£¼ì œ ì„¤ì • í”„ë¡¬í”„íŠ¸ë¥¼ ì‹œìŠ¤í…œ/ì‚¬ìš©ì ë©”ì‹œì§€ë¡œ ë¶„ë¦¬"""
    system_message = (
        "ë‹¹ì‹ ì€ ë°ì´í„° ì¸ê³¼ê´€ê³„ ë¶„ì„ í”„ë¡œì íŠ¸ì˜ ì£¼ì œë¥¼ ì„¤ì •í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
        "ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ëª…í™•í•œ ë¶„ì„ ëª©í‘œë¥¼ ì„¤ì •í•˜ê³ , "
        "í•„ìš”í•œ ë°ì´í„°ì˜ ì¢…ë¥˜ì™€ íŠ¹ì„±, ê·¸ë¦¬ê³  ê¸°ëŒ€ë˜ëŠ” ì¸ê³¼ê´€ê³„ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”. "
        "ì½”ë“œ êµ¬í˜„ì€ í•˜ì§€ ë§ê³  ë¶„ì„ ê³„íšê³¼ ë°ì´í„° ìš”êµ¬ì‚¬í•­ì— ì§‘ì¤‘í•´ì£¼ì„¸ìš”."
    )

    human_message = (
        f"{topic_prompt}\n\n"
        f"ìœ„ ìš”ì²­ì— ëŒ€í•´ ë‹¤ìŒ ì‚¬í•­ë“¤ì„ í¬í•¨í•˜ì—¬ ë¶„ì„ ê³„íšì„ ì„¸ì›Œì£¼ì„¸ìš”:\n"
        f"1. ë¶„ì„ ëª©í‘œì™€ ê¸°ëŒ€ë˜ëŠ” ì¸ê³¼ê´€ê³„\n"
        f"2. í•„ìš”í•œ ë°ì´í„°ì˜ ì¢…ë¥˜ì™€ ë³€ìˆ˜\n"
        f"3. ì£¼ìš” ë¶„ì„ ê´€ì  (ë…ë¦½ë³€ìˆ˜, ì¢…ì†ë³€ìˆ˜, ì¡°ì ˆë³€ìˆ˜ ë“±)\n"
        f"4. ì˜ˆìƒë˜ëŠ” ì‹œê°í™” ë°©í–¥"
    )

    return system_message, human_message


##############################
# Data processing prompts    #
##############################
def auto_process_data_prompt(df: pd.DataFrame) -> Tuple[str, str]:
    """ë°ì´í„° ì „ì²˜ë¦¬ í†µí•© í”„ë¡¬í”„íŠ¸ - ì»¬ëŸ¼ ì‚­ì œ ì¶”ì²œ + ì˜ë¬¸ ë³€í™˜ì„ í•œë²ˆì— ì²˜ë¦¬"""
    system_message = (
        "ë‹¹ì‹ ì€ ë°ì´í„° ì „ì²˜ë¦¬ ë° ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
        "ì œê³µëœ ë°ì´í„°í”„ë ˆì„ì„ ë¶„ì„í•˜ì—¬ ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ì„ ì‹ë³„í•˜ê³ , "
        "ë‚¨ì€ ì»¬ëŸ¼ë“¤ì„ ì˜ë¬¸ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì‘ì—…ì„ í•œë²ˆì— ìˆ˜í–‰í•´ì£¼ì„¸ìš”.\n\n"

        "**ğŸš¨ í•„ìˆ˜ ìš”êµ¬ì‚¬í•­ (ì ˆëŒ€ ì¤€ìˆ˜):**\n"
        "1. ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì‹ë³„ ë° ì œê±° (ìµœëŒ€ 5ê°œ)\n"
        "2. ë‚¨ì€ ëª¨ë“  ì»¬ëŸ¼ëª…ì„ ë°˜ë“œì‹œ ì˜ë¬¸ìœ¼ë¡œë§Œ ë³€í™˜\n"
        "3. í•œê¸€ëª… ì‚¬ìš© ì ˆëŒ€ ê¸ˆì§€ - ì–´ë–¤ ê²½ìš°ì—ë„ í•œê¸€ í¬í•¨ ë¶ˆê°€\n"
        "4. ê²°ê³¼ dictionaryì˜ valueëŠ” 100% ì˜ë¬¸ëª…ë§Œ í—ˆìš©\n\n"

        "**ì»¬ëŸ¼ ì œê±° ê¸°ì¤€ (ìµœëŒ€ 5ê°œê¹Œì§€ë§Œ):**\n"
        "1. ì¤‘ë³µ ì •ë³´ (ë‹¤ë¥¸ ì»¬ëŸ¼ê³¼ ë™ì¼í•œ ì˜ë¯¸)\n"
        "2. ë†’ì€ ê²°ì¸¡ë¥  (70% ì´ìƒ NULL)\n"
        "3. ë‹¨ì¼ê°’ ì»¬ëŸ¼ (ëª¨ë“  í–‰ì´ ë™ì¼í•œ ê°’)\n"
        "4. ì„ì‹œ ì‹ë³„ì (ì„ì‹œ ID, ì¸ë±ìŠ¤ ë²ˆí˜¸)\n"
        "5. ë©”íƒ€ë°ì´í„° (íŒŒì¼ëª…, ìƒì„±ì¼ì‹œ ë“± ë¶„ì„ ë¬´ê´€ ì •ë³´)\n"
        "** ì‚­ì œ ê¸°ì¤€ì— í•´ë‹¹í•˜ë”ë¼ë„ ì¸ê³¼ê´€ê³„ ë¶„ì„ì— ì¤‘ìš”í•œ ì»¬ëŸ¼ì€ ë³´ì¡´ **\n\n"

        "**ë°ì´í„°ë² ì´ìŠ¤ ì»¬ëŸ¼ëª… ë³€í™˜ ê·œì¹™:**\n"
        "1. ë°˜ë“œì‹œ ì˜ë¬¸ ì¶•ì•½ì–´ ë˜ëŠ” ì˜ë¬¸ ë‹¨ì–´ ì‚¬ìš© (ISO/ANSI í‘œì¤€)\n"
        "2. ì†Œë¬¸ì + ì–¸ë”ìŠ¤ì½”ì–´ í˜•ì‹ (snake_case)\n"
        "3. ìµœëŒ€ 30ì ì´ë‚´ ê¶Œì¥\n"
        "4. ìˆ«ìë¡œ ì‹œì‘ ê¸ˆì§€\n"
        "5. íŠ¹ìˆ˜ë¬¸ìëŠ” ì–¸ë”ìŠ¤ì½”ì–´(_)ë§Œ í—ˆìš©\n\n"

        "**í‘œì¤€ ì¶•ì•½ì–´ ê°€ì´ë“œ:**\n"
        "- ì‹ë³„ì: id, seq, no, code, key\n"
        "- ë‚ ì§œ/ì‹œê°„: dt, tm, ts, yr, mon, dy, created_at, updated_at\n"
        "- ì´ë¦„: nm, title, desc, label\n"
        "- ìƒíƒœ: stat, flag, ind, active_yn\n"
        "- ìˆ˜ëŸ‰: qty, cnt, amt, val, rate, pct, total\n"
        "- ë¶„ë¥˜: cat, type, cls, grp, div\n"
        "- ìœ„ì¹˜: pos, loc, coord, lat, lng\n"
        "- ì¬ì •: price, cost, fee, tax, revenue, profit\n\n"

        "**ì¶œë ¥ í˜•ì‹ (ì—„ê²©íˆ ì¤€ìˆ˜):**\n"
        "ì²« ë²ˆì§¸ ì¤„ì—ë§Œ Python dictionary í˜•íƒœë¡œ ë°˜í™˜:\n"
        "- key: ì›ë³¸ í•œê¸€ ì»¬ëŸ¼ëª… (ì‚­ì œí•  ì»¬ëŸ¼ ì œì™¸)\n"
        "- value: ë³€í™˜ëœ ì˜ë¬¸ ì»¬ëŸ¼ëª…\n"
        "- ì‚­ì œí•  ì»¬ëŸ¼ì€ dictionaryì—ì„œ ì™„ì „íˆ ì œì™¸\n"
        "- ì¤„ë°”ê¿ˆ, ì„¤ëª…, ì¶”ê°€ í…ìŠ¤íŠ¸ ì ˆëŒ€ ê¸ˆì§€\n\n"

        "**ì˜ˆì‹œ:**\n"
        "ì…ë ¥ ì»¬ëŸ¼: ['ID', 'íŒë§¤ëŸ‰', 'ë§ˆì¼€íŒ… ì˜ˆì‚°', 'ê¸°ì˜¨', 'ë¶ˆí•„ìš”í•œì»¬ëŸ¼']\n"
        "ì¶œë ¥: {'íŒë§¤ëŸ‰': 'sales_qty', 'ë§ˆì¼€íŒ… ì˜ˆì‚°': 'marketing_budget', 'ê¸°ì˜¨': 'temperature'}\n"
        "(IDì™€ ë¶ˆí•„ìš”í•œì»¬ëŸ¼ì€ ì œê±°ë˜ì–´ dictionaryì— ì—†ìŒ)"
    )

    preview = df.head(3).to_string(index=False)
    column_list = ", ".join(df.columns)

    # ì»¬ëŸ¼ë³„ í†µê³„ ì •ë³´
    column_stats = []
    for col in df.columns:
        null_pct = (df[col].isnull().sum() / len(df)) * 100
        unique_count = df[col].nunique()
        column_stats.append(f"  - {col}: ê²°ì¸¡ë¥  {null_pct:.1f}%, ê³ ìœ ê°’ {unique_count}ê°œ")

    stats_info = "\n".join(column_stats)

    human_message = (
        f"ë‹¤ìŒ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ì„ ì œê±°í•˜ê³ , ë‚¨ì€ ì»¬ëŸ¼ì„ ì˜ë¬¸ìœ¼ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.\n\n"
        f"**ì»¬ëŸ¼ ëª©ë¡:**\n{column_list}\n\n"
        f"**ì»¬ëŸ¼ í†µê³„:**\n{stats_info}\n\n"
        f"**ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ìƒìœ„ 3ê°œ í–‰):**\n```\n{preview}\n```\n\n"
        f"âš ï¸ ì¤‘ìš”: \n"
        f"1. ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ì€ dictionaryì—ì„œ ì™„ì „íˆ ì œì™¸\n"
        f"2. ë‚¨ì€ ì»¬ëŸ¼ë§Œ ì˜ë¬¸ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ dictionaryì— í¬í•¨\n"
        f"3. valueëŠ” 100% ì˜ë¬¸ëª…ë§Œ ì‚¬ìš©\n\n"
        f"ì²« ë²ˆì§¸ ì¤„ì— {{ì›ë³¸í•œê¸€ëª…: ì˜ë¬¸ëª…}} í˜•íƒœì˜ dictionaryë§Œ ì¶œë ¥í•˜ì„¸ìš”."
    )

    return system_message, human_message


#######################################
# Visualization recommendation prompts#
#######################################
def recommend_visualization_template_prompt(
        df: pd.DataFrame, user_context: Optional[str] = None
) -> Tuple[str, str]:
    """ì‹œê°í™” í…œí”Œë¦¿ ì¶”ì²œ í”„ë¡¬í”„íŠ¸ë¥¼ ì‹œìŠ¤í…œ/ì‚¬ìš©ì ë©”ì‹œì§€ë¡œ ë¶„ë¦¬"""
    system_message = (
        "ë‹¹ì‹ ì€ ë°ì´í„° ì‹œê°í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
        "ì œê³µëœ ë°ì´í„°ì˜ íŠ¹ì„±ê³¼ ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ì„ ë¶„ì„í•˜ì—¬ ì¸ê³¼ê´€ê³„ ë¶„ì„ì— ìµœì ì˜ ì‹œê°í™” í…œí”Œë¦¿ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”.\n\n"

        "**ğŸš¨ ì¤‘ìš”: ì‘ë‹µì€ ë°˜ë“œì‹œ JSON í˜•ì‹ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ë‚˜ ì„¤ëª…ì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.**\n\n"

        "**ì‚¬ìš© ê°€ëŠ¥í•œ í…œí”Œë¦¿ (visualization_typeì— ì •í™•íˆ ì´ ê°’ ì‚¬ìš©):**\n"
        "- time-series: ì‹œê³„ì—´ ë¶„ì„ [êµ¬í˜„ë¨]\n"
        "- kpi-dashboard: KPI ëŒ€ì‹œë³´ë“œ [êµ¬í˜„ë¨]\n"
        "- comparison: ë¹„êµ ë¶„ì„ [êµ¬í˜„ë¨]\n"
        "- geo-spatial: ì§€ë„ ì‹œê°í™” [ê°œë°œ ì˜ˆì •]\n"
        "- gantt-chart: í”„ë¡œì íŠ¸ ì¼ì • [ê°œë°œ ì˜ˆì •]\n"
        "- heatmap: íˆíŠ¸ë§µ [ê°œë°œ ì˜ˆì •]\n"
        "- network-graph: ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ [ê°œë°œ ì˜ˆì •]\n"
        "- custom: ì‚¬ìš©ì ì •ì˜ [ê°œë°œ ì˜ˆì •]\n\n"

        "**í•„ìˆ˜ ì¶œë ¥ í˜•ì‹ (ì •í™•íˆ ì´ êµ¬ì¡°ë¥¼ ë”°ë¥´ì„¸ìš”):**\n"
        "```json\n"
        "[\n"
        "  {\n"
        '    "visualization_type": "time-series",\n'
        '    "confidence": 95,\n'
        '    "reason": "ì‹œê³„ì—´ ë°ì´í„°ê°€ í¬í•¨ë˜ì–´ íŠ¸ë Œë“œ ë¶„ì„ì— ì í•©",\n'
        '    "data_requirements": ["date", "value"],\n'
        '    "benefits": ["ì‹œê°„ì— ë”°ë¥¸ ë³€í™” ì¶”ì ", "íŒ¨í„´ ë°œê²¬ ìš©ì´"]\n'
        "  }\n"
        "]\n"
        "```\n\n"

        "**ì¶œë ¥ ê·œì¹™:**\n"
        "1. JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥ (ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ê¸ˆì§€)\n"
        "2. visualization_typeì€ ìœ„ í…œí”Œë¦¿ ëª©ë¡ì—ì„œ ì •í™•í•œ ê°’ ì‚¬ìš©\n"
        "3. confidenceëŠ” 0-100 ì‚¬ì´ ì •ìˆ˜\n"
        "4. êµ¬í˜„ëœ í…œí”Œë¦¿(time-series, kpi-dashboard, comparison) ìš°ì„  ì¶”ì²œ\n"
        "5. ìµœëŒ€ 3ê°œê¹Œì§€ ì¶”ì²œ (ì‹ ë¢°ë„ ë†’ì€ ìˆœ)\n"
        "6. ì²« ë²ˆì§¸ ì¤„ë¶€í„° ë§ˆì§€ë§‰ ì¤„ê¹Œì§€ JSONë§Œ ì¶œë ¥"
    )

    # ë°ì´í„° ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘
    preview = df.head(3).to_string(index=False)
    column_info = []

    for col in df.columns:
        dtype = str(df[col].dtype)
        null_count = df[col].isnull().sum()
        unique_count = df[col].nunique()
        sample_values = df[col].dropna().head(3).tolist()
        column_info.append(f"- {col}: {dtype} (ê³ ìœ ê°’: {unique_count}, ê²°ì¸¡ê°’: {null_count}) ì˜ˆì‹œ: {sample_values}")

    column_analysis = "\n".join(column_info)

    # ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬
    context_section = ""
    if user_context:
        context_section = f"\n[ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ ë° ë¶„ì„ ëª©ì ]\n{user_context}\n"

    human_message = (
        f"[ë°ì´í„° ì •ë³´]\n"
        f"- ì´ í–‰ ìˆ˜: {len(df):,}\n"
        f"- ì´ ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}\n\n"
        f"[ì»¬ëŸ¼ ìƒì„¸ ì •ë³´]\n{column_analysis}\n"
        f"{context_section}"
        f"[ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ìƒìœ„ 3ê°œ í–‰)]\n```\n{preview}\n```\n\n"
        f"âš ï¸ ì¤‘ìš”: ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ì—†ì´ JSON ë°°ì—´ë§Œ ì¶œë ¥í•˜ì„¸ìš”.\n"
        f"ì²« ë²ˆì§¸ ë¬¸ìëŠ” ë°˜ë“œì‹œ '[' ì´ì–´ì•¼ í•˜ê³ , ë§ˆì§€ë§‰ ë¬¸ìëŠ” ']' ì´ì–´ì•¼ í•©ë‹ˆë‹¤."
    )

    return system_message, human_message
